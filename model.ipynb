{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import pickle\n",
    "import re\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pd.read_csv('/Users/lucashawranke/Documents/data-science/sarcasm-scanner/data/real_tweets.csv')\n",
    "satire = pd.read_csv('/Users/lucashawranke/Documents/data-science/sarcasm-scanner/data/satire_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making lists to append all tweets/data to\n",
    "# real = []\n",
    "# satire = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sntwitter to scrape tweets and append data to lists\n",
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:cnnbrk').get_items()):\n",
    "#     if i>5000:\n",
    "#         break\n",
    "#     real.append([tweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:enews').get_items()):\n",
    "#     if i>5000:\n",
    "#         break\n",
    "#     real.append([tweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real = pd.DataFrame(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real.to_csv('/Users/lucashawranke/Documents/data-science/sarcasm-scanner/data/real_tweets.csv') #mac\n",
    "# real.to_csv('/Users/lucas/data-science/sarcasm-scanner/data/real_tweets.csv') #pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:theonion').get_items()):\n",
    "#     if i>5000:\n",
    "#         break\n",
    "#     satire.append([tweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:clickhole').get_items()):\n",
    "#     if i>5000:\n",
    "#         break\n",
    "#     satire.append([tweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# satire = pd.DataFrame(satire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# satire.to_csv('/Users/lucashawranke/documents/data-science/sarcasm-scanner/data/satire_tweets.csv') #pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "real['Authenticity'] = 'real'\n",
    "satire['Authenticity'] = 'satire'\n",
    "df = pd.concat([real, satire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'0': 'Tweets'}, inplace=True)\n",
    "df.rename(columns={'Unnamed: 0': 'No.'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Authenticity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Cannon is getting some rest after coming ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Team USA eliminated from the World Cup after a...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A man has been arrested and charged with murde...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Right-wing conspiracy theorist Alex Jones file...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>America added a robust 263,000 jobs last month...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>Are You A Widow? https://t.co/UWZLPewsKD https...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>Michael B. Jordan Said What?! https://t.co/y4F...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>Did ‘Sesame Street’ Go Too Far With Its Episod...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>Close Fucking Call: This Product That Got A Ne...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>10001</td>\n",
       "      <td>Oh fuck. https://t.co/HQEBco5XzS</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20004 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         No.                                             Tweets Authenticity\n",
       "0          0  Nick Cannon is getting some rest after coming ...         real\n",
       "1          1  Team USA eliminated from the World Cup after a...         real\n",
       "2          2  A man has been arrested and charged with murde...         real\n",
       "3          3  Right-wing conspiracy theorist Alex Jones file...         real\n",
       "4          4  America added a robust 263,000 jobs last month...         real\n",
       "...      ...                                                ...          ...\n",
       "9997    9997  Are You A Widow? https://t.co/UWZLPewsKD https...       satire\n",
       "9998    9998  Michael B. Jordan Said What?! https://t.co/y4F...       satire\n",
       "9999    9999  Did ‘Sesame Street’ Go Too Far With Its Episod...       satire\n",
       "10000  10000  Close Fucking Call: This Product That Got A Ne...       satire\n",
       "10001  10001                   Oh fuck. https://t.co/HQEBco5XzS       satire\n",
       "\n",
       "[20004 rows x 3 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets'] = df['Tweets'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Authenticity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Cannon is getting some rest after coming ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Team USA eliminated from the World Cup after a...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A man has been arrested and charged with murde...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Right-wing conspiracy theorist Alex Jones file...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>America added a robust 263,000 jobs last month...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>Are You A Widow?</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>Michael B. Jordan Said What?!</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>Did ‘Sesame Street’ Go Too Far With Its Episod...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>Close Fucking Call: This Product That Got A Ne...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>10001</td>\n",
       "      <td>Oh fuck.</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20004 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         No.                                             Tweets Authenticity\n",
       "0          0  Nick Cannon is getting some rest after coming ...         real\n",
       "1          1  Team USA eliminated from the World Cup after a...         real\n",
       "2          2  A man has been arrested and charged with murde...         real\n",
       "3          3  Right-wing conspiracy theorist Alex Jones file...         real\n",
       "4          4  America added a robust 263,000 jobs last month...         real\n",
       "...      ...                                                ...          ...\n",
       "9997    9997                                  Are You A Widow?        satire\n",
       "9998    9998                     Michael B. Jordan Said What?!        satire\n",
       "9999    9999  Did ‘Sesame Street’ Go Too Far With Its Episod...       satire\n",
       "10000  10000  Close Fucking Call: This Product That Got A Ne...       satire\n",
       "10001  10001                                          Oh fuck.        satire\n",
       "\n",
       "[20004 rows x 3 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/lucashawranke/Documents/data-science/sarcasm-scanner/data/df.csv') #mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating/Running MultinomialNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = real[:8000]\n",
    "X_test = real[-2000:]\n",
    "\n",
    "y_train = satire[:8000]\n",
    "y_test = satire[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9087728067983004\n",
      "confusion matrix:  [[1925   91]\n",
      " [ 274 1711]]\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('nb', mnb)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix: ', confusion_matrix(test['Authenticity'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, this model has an accuracy of 90.8%. Although we might be able to do better, let's test it out and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['satire']\n"
     ]
    }
   ],
   "source": [
    "headline = np.array([[\"ISIS Takes Credit For Relatives Overstaying Welcome At Thanksgiving\"]])\n",
    "predictions = pipe.predict(headline[0])\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct! This article is from [a different](https://web.archive.org/web/20221204015813/http://www.cap-news.com/) satire news site. Let's save the model either way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lucashawranke/Documents/data-science/sarcasm-scanner/mnb.pkl']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe, '/Users/lucashawranke/Documents/data-science/sarcasm-scanner/mnb.pkl') #mac\n",
    "# joblib.dump(pipe, '/Users/lucas/data-science/sarcasm-scanner/model.pkl') #pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mac\n",
    "# with open('/Users/lucashawranke/Documents/data-science/sarcasm-scanner/model.pkl', 'wb') as files:\n",
    "#     pickle.dump(pipe, files)\n",
    "#pc\n",
    "# with open('/Users/lucas/data-science/sarcasm-scanner/model.pkl', 'wb') as files:\n",
    "#     pickle.dump(pipe, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9492626843289178\n",
      "confusion matrix: [[1890  126]\n",
      " [  77 1908]]\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('svc', svc)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix:', confusion_matrix(test['Authenticity'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lucashawranke/documents/data-science/sarcasm-scanner/lsvc.pkl']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe, '/Users/lucashawranke/documents/data-science/sarcasm-scanner/lsvc.pkl') #mac\n",
    "# joblib.dump(pipe, '/Users/lucas/data-science/sarcasm-scanner/model2.pkl') #pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.939265183704074\n",
      "confusion matrix: [[1855  161]\n",
      " [  82 1903]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('lr', lr)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix:', confusion_matrix(test['Authenticity'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9232691827043239\n",
      "confusion matrix: [[1822  194]\n",
      " [ 113 1872]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('rf', rf)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix:', confusion_matrix(test['Authenticity'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8470382404398901\n",
      "confusion matrix: [[1575  441]\n",
      " [ 171 1814]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('dt', dt)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix:', confusion_matrix(test['Authenticity'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8780304923769058\n",
      "confusion matrix: [[1843  173]\n",
      " [ 315 1670]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('knn', knn)])\n",
    "pipe.fit(train.Tweets, train.Authenticity)\n",
    "predictions = pipe.predict(test.Tweets)\n",
    "print('accuracy: ', accuracy_score(test['Authenticity'], predictions))\n",
    "print('confusion matrix:', confusion_matrix(test['Authenticity'], predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cc3b03a9a7e4010a291141970bf1cb2053c725e17844b2b26603859505c2b66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
